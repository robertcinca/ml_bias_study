{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import bootstrap  # noqa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ml_bias_explainability.explain_bias import ExplainBias\n",
    "from ml_bias_explainability.bias_analysis import BiasAnalysis\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove=[\n",
    "    # output column\n",
    "    # \"two_year_recid\",\n",
    "    # duplicate columns\n",
    "    \"decile_score.1\",\n",
    "    \"priors_count.1\",\n",
    "    # Not relevant\n",
    "    \"name\",\n",
    "    \"first\",\n",
    "    \"last\",\n",
    "    \"compas_screening_date\",\n",
    "    \"days_b_screening_arrest\",\n",
    "    \"c_jail_in\",\n",
    "    \"c_jail_out\",\n",
    "    \"c_case_number\",\n",
    "    \"c_offense_date\",\n",
    "    \"c_arrest_date\",\n",
    "    \"c_days_from_compas\",\n",
    "    \"type_of_assessment\",\n",
    "    \"screening_date\",\n",
    "    \"v_type_of_assessment\",\n",
    "    \"v_screening_date\",\n",
    "    \"in_custody\",\n",
    "    \"out_custody\",\n",
    "    # too similar with other features\n",
    "    \"dob\",\n",
    "    # too similar with output feature\n",
    "    \"decile_score\",\n",
    "    \"is_recid\",\n",
    "    \"r_case_number\",\n",
    "    \"r_charge_degree\",\n",
    "    \"r_days_from_arrest\",\n",
    "    \"r_offense_date\",\n",
    "    \"r_charge_desc\",\n",
    "    \"r_jail_in\",\n",
    "    \"r_jail_out\",\n",
    "    \"violent_recid\",\n",
    "    \"is_violent_recid\",\n",
    "    \"vr_case_number\",\n",
    "    \"vr_charge_degree\",\n",
    "    \"vr_offense_date\",\n",
    "    \"vr_charge_desc\",\n",
    "    \"score_text\",\n",
    "    \"v_decile_score\",\n",
    "    \"v_score_text\",\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"event\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Aggravated Assault w/Firearm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Felony Battery w/Prior Convict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Cocaine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Battery</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession Burglary Tools</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Deliver Cannabis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Leaving the Scene of Accident</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>Male</td>\n",
       "      <td>57</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Aggravated Battery / Pregnant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>Battery on Law Enforc Officer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11001</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Ethylone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sex  age          age_cat              race  juv_fel_count  \\\n",
       "id                                                                     \n",
       "1        Male   69  Greater than 45             Other              0   \n",
       "3        Male   34          25 - 45  African-American              0   \n",
       "4        Male   24     Less than 25  African-American              0   \n",
       "7        Male   44          25 - 45             Other              0   \n",
       "8        Male   41          25 - 45         Caucasian              0   \n",
       "...       ...  ...              ...               ...            ...   \n",
       "10996    Male   23     Less than 25  African-American              0   \n",
       "10997    Male   23     Less than 25  African-American              0   \n",
       "10999    Male   57  Greater than 45             Other              0   \n",
       "11000  Female   33          25 - 45  African-American              0   \n",
       "11001  Female   23     Less than 25          Hispanic              0   \n",
       "\n",
       "       juv_misd_count  juv_other_count  priors_count c_charge_degree  \\\n",
       "id                                                                     \n",
       "1                   0                0             0               F   \n",
       "3                   0                0             0               F   \n",
       "4                   0                1             4               F   \n",
       "7                   0                0             0               M   \n",
       "8                   0                0            14               F   \n",
       "...               ...              ...           ...             ...   \n",
       "10996               0                0             0               F   \n",
       "10997               0                0             0               F   \n",
       "10999               0                0             0               F   \n",
       "11000               0                0             3               M   \n",
       "11001               0                0             2               F   \n",
       "\n",
       "                        c_charge_desc  two_year_recid  \n",
       "id                                                     \n",
       "1        Aggravated Assault w/Firearm               0  \n",
       "3      Felony Battery w/Prior Convict               1  \n",
       "4               Possession of Cocaine               1  \n",
       "7                             Battery               0  \n",
       "8           Possession Burglary Tools               1  \n",
       "...                               ...             ...  \n",
       "10996                Deliver Cannabis               0  \n",
       "10997   Leaving the Scene of Accident               0  \n",
       "10999   Aggravated Battery / Pregnant               0  \n",
       "11000   Battery on Law Enforc Officer               0  \n",
       "11001          Possession of Ethylone               1  \n",
       "\n",
       "[6172 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "def read_data():\n",
    "        # The dataset used is taken from https://github.com/propublica/compas-analysis\n",
    "        df = pd.read_csv(\"data/compas-scores-two-years.csv\", index_col=0)\n",
    "\n",
    "        # Filter certain rows based on propublica analysis\n",
    "        df = df[df.days_b_screening_arrest <= 30]\n",
    "        df = df[df.days_b_screening_arrest >= -30]\n",
    "        df = df[df.is_recid != -1]\n",
    "        df = df[df.c_charge_degree != \"O\"]\n",
    "        df = df[df.score_text != \"N/A\"]\n",
    "\n",
    "        # get rid of columns that shouldn't be in the input model\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "        # Ensure validity of inputs â€“ No NaNs\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "output_column = \"two_year_recid\"\n",
    "df = read_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Check for correlations between features to know which ones to remove from dataframe \n",
    "# # We don't want ones that are too similar to the output feature, i.e. a proxy of the output\n",
    "\n",
    "# correlation_matrix = df.corr().abs()\n",
    "\n",
    "# relevant_correlations = (\n",
    "#     correlation_matrix.where(\n",
    "#         np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool)\n",
    "#     )\n",
    "#     .stack()\n",
    "#     .sort_values(ascending=False)\n",
    "# )\n",
    "\n",
    "# with pd.option_context('display.max_rows', 1000, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(relevant_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data for DL model\n",
    "def convert_df_to_model_input(df, columns_to_remove, output_column):\n",
    "    # extract output feature\n",
    "    y_dataset = to_categorical(df[output_column])\n",
    "    \n",
    "    # get rid of columns that shouldn't be in the input model\n",
    "#     df = df.drop(columns=columns_to_remove)\n",
    "    \n",
    "    # Convert df to an x_train tensorflow-input format \n",
    "    # numeric followed by categorical columns\n",
    "    numeric_list = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_list] = df[numeric_list].astype(np.float32)\n",
    "\n",
    "    object_list = df.select_dtypes(object).columns\n",
    "    for column in object_list:\n",
    "        df[column] = pd.Categorical(df[column])\n",
    "        df[column] = df[column].cat.codes\n",
    "\n",
    "    # extract input features, replace3 NAN with -1\n",
    "    x_dataset = to_categorical(np.where(np.isnan(df.values), -1, df.values))\n",
    "    \n",
    "    # flatted array into one dimension per sample\n",
    "    x_dataset = x_dataset.reshape((x_dataset.shape[0], -1)) \n",
    "    \n",
    "    return x_dataset, y_dataset\n",
    "\n",
    "output_column = \"two_year_recid\"\n",
    "x_dataset, y_dataset = convert_df_to_model_input(df, columns_to_remove, output_column)\n",
    "x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into several arrays\n",
    "total_length = len(x_dataset)\n",
    "sixty_percent = round(total_length*0.6)\n",
    "eighty_percent = round(total_length*0.8)\n",
    "\n",
    "x_train = x_dataset[:sixty_percent]\n",
    "x_val = x_dataset[sixty_percent+1:eighty_percent]\n",
    "x_test = x_dataset[eighty_percent+1:]\n",
    "\n",
    "y_train = y_dataset[:sixty_percent]\n",
    "y_val = y_dataset[sixty_percent+1:eighty_percent]\n",
    "y_test = y_dataset[eighty_percent+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "8/8 [==============================] - 1s 35ms/step - loss: 0.6826 - acc: 0.5128 - val_loss: 0.6517 - val_acc: 0.5389\n",
      "Epoch 2/4\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6402 - acc: 0.5621 - val_loss: 0.6194 - val_acc: 0.6053\n",
      "Epoch 3/4\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6032 - acc: 0.6819 - val_loss: 0.5776 - val_acc: 0.7869\n",
      "Epoch 4/4\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5630 - acc: 0.8260 - val_loss: 0.5436 - val_acc: 0.8201\n",
      "39/39 [==============================] - 0s 625us/step - loss: 0.5404 - acc: 0.8264\n"
     ]
    }
   ],
   "source": [
    "# Build the network\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(len(x_train[0]),)))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# compile the network\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# 4\n",
    "history = model.fit(x_train, y_train, epochs=4, batch_size=512, validation_data=(x_val, y_val))\n",
    "# history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5403616428375244, 0.8264395594596863]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53 0.47\n",
      "0.63 0.37\n",
      "0.63 0.37\n",
      "0.68 0.32\n",
      "0.54 0.46\n",
      "0.57 0.43\n",
      "0.6 0.4\n",
      "0.66 0.34\n",
      "0.49 0.51\n",
      "0.58 0.42\n",
      "0.64 0.36\n",
      "0.58 0.42\n",
      "0.59 0.41\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.52 0.48\n",
      "0.65 0.35\n",
      "0.65 0.35\n",
      "0.66 0.34\n",
      "0.66 0.34\n",
      "0.53 0.47\n",
      "0.49 0.51\n",
      "0.52 0.48\n",
      "0.59 0.41\n",
      "0.65 0.35\n",
      "0.51 0.49\n",
      "0.71 0.29\n",
      "0.5 0.5\n",
      "0.6 0.4\n",
      "0.67 0.33\n",
      "0.55 0.45\n",
      "0.66 0.34\n",
      "0.53 0.47\n",
      "0.56 0.44\n",
      "0.68 0.32\n",
      "0.63 0.37\n",
      "0.53 0.47\n",
      "0.58 0.42\n",
      "0.56 0.44\n",
      "0.63 0.37\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.57 0.43\n",
      "0.64 0.36\n",
      "0.57 0.43\n",
      "0.64 0.36\n",
      "0.64 0.36\n",
      "0.68 0.32\n",
      "0.53 0.47\n",
      "0.69 0.31\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.55 0.45\n",
      "0.63 0.37\n",
      "0.59 0.41\n",
      "0.56 0.44\n",
      "0.52 0.48\n",
      "0.69 0.31\n",
      "0.5 0.5\n",
      "0.52 0.48\n",
      "0.63 0.37\n",
      "0.6 0.4\n",
      "0.67 0.33\n",
      "0.62 0.38\n",
      "0.53 0.47\n",
      "0.63 0.37\n",
      "0.68 0.32\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.73 0.27\n",
      "0.53 0.47\n",
      "0.5 0.5\n",
      "0.54 0.46\n",
      "0.5 0.5\n",
      "0.62 0.38\n",
      "0.65 0.35\n",
      "0.51 0.49\n",
      "0.59 0.41\n",
      "0.51 0.49\n",
      "0.54 0.46\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.68 0.32\n",
      "0.63 0.37\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.52 0.48\n",
      "0.63 0.37\n",
      "0.58 0.42\n",
      "0.64 0.36\n",
      "0.65 0.35\n",
      "0.7 0.3\n",
      "0.5 0.5\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.71 0.29\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.57 0.43\n",
      "0.56 0.44\n",
      "0.58 0.42\n",
      "0.62 0.38\n",
      "0.69 0.31\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.62 0.38\n",
      "0.59 0.41\n",
      "0.63 0.37\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.6 0.4\n",
      "0.68 0.32\n",
      "0.62 0.38\n",
      "0.63 0.37\n",
      "0.67 0.33\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.54 0.46\n",
      "0.49 0.51\n",
      "0.7 0.3\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.67 0.33\n",
      "0.6 0.4\n",
      "0.52 0.48\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.55 0.45\n",
      "0.69 0.31\n",
      "0.61 0.39\n",
      "0.52 0.48\n",
      "0.53 0.47\n",
      "0.67 0.33\n",
      "0.65 0.35\n",
      "0.68 0.32\n",
      "0.67 0.33\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.52 0.48\n",
      "0.65 0.35\n",
      "0.49 0.51\n",
      "0.7 0.3\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.58 0.42\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.59 0.41\n",
      "0.55 0.45\n",
      "0.55 0.45\n",
      "0.56 0.44\n",
      "0.59 0.41\n",
      "0.56 0.44\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.67 0.33\n",
      "0.57 0.43\n",
      "0.58 0.42\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.56 0.44\n",
      "0.51 0.49\n",
      "0.53 0.47\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.53 0.47\n",
      "0.52 0.48\n",
      "0.67 0.33\n",
      "0.49 0.51\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.68 0.32\n",
      "0.55 0.45\n",
      "0.62 0.38\n",
      "0.6 0.4\n",
      "0.55 0.45\n",
      "0.57 0.43\n",
      "0.55 0.45\n",
      "0.58 0.42\n",
      "0.62 0.38\n",
      "0.52 0.48\n",
      "0.6 0.4\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.54 0.46\n",
      "0.53 0.47\n",
      "0.61 0.39\n",
      "0.49 0.51\n",
      "0.6 0.4\n",
      "0.6 0.4\n",
      "0.62 0.38\n",
      "0.52 0.48\n",
      "0.62 0.38\n",
      "0.66 0.34\n",
      "0.57 0.43\n",
      "0.57 0.43\n",
      "0.53 0.47\n",
      "0.64 0.36\n",
      "0.55 0.45\n",
      "0.55 0.45\n",
      "0.56 0.44\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.63 0.37\n",
      "0.64 0.36\n",
      "0.69 0.31\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.51 0.49\n",
      "0.54 0.46\n",
      "0.63 0.37\n",
      "0.64 0.36\n",
      "0.67 0.33\n",
      "0.6 0.4\n",
      "0.52 0.48\n",
      "0.67 0.33\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.6 0.4\n",
      "0.63 0.37\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.69 0.31\n",
      "0.55 0.45\n",
      "0.61 0.39\n",
      "0.65 0.35\n",
      "0.63 0.37\n",
      "0.54 0.46\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.68 0.32\n",
      "0.55 0.45\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.73 0.27\n",
      "0.53 0.47\n",
      "0.63 0.37\n",
      "0.66 0.34\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.71 0.29\n",
      "0.65 0.35\n",
      "0.67 0.33\n",
      "0.62 0.38\n",
      "0.62 0.38\n",
      "0.59 0.41\n",
      "0.64 0.36\n",
      "0.58 0.42\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.49 0.51\n",
      "0.63 0.37\n",
      "0.49 0.51\n",
      "0.63 0.37\n",
      "0.7 0.3\n",
      "0.54 0.46\n",
      "0.67 0.33\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.66 0.34\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.53 0.47\n",
      "0.53 0.47\n",
      "0.58 0.42\n",
      "0.52 0.48\n",
      "0.68 0.32\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.54 0.46\n",
      "0.61 0.39\n",
      "0.58 0.42\n",
      "0.56 0.44\n",
      "0.62 0.38\n",
      "0.52 0.48\n",
      "0.69 0.31\n",
      "0.58 0.42\n",
      "0.55 0.45\n",
      "0.56 0.44\n",
      "0.54 0.46\n",
      "0.55 0.45\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.49 0.51\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.68 0.32\n",
      "0.68 0.32\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.62 0.38\n",
      "0.66 0.34\n",
      "0.54 0.46\n",
      "0.58 0.42\n",
      "0.66 0.34\n",
      "0.53 0.47\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.65 0.35\n",
      "0.69 0.31\n",
      "0.55 0.45\n",
      "0.57 0.43\n",
      "0.52 0.48\n",
      "0.54 0.46\n",
      "0.58 0.42\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.54 0.46\n",
      "0.51 0.49\n",
      "0.59 0.41\n",
      "0.66 0.34\n",
      "0.51 0.49\n",
      "0.62 0.38\n",
      "0.61 0.39\n",
      "0.58 0.42\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.63 0.37\n",
      "0.54 0.46\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.54 0.46\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.62 0.38\n",
      "0.69 0.31\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.53 0.47\n",
      "0.56 0.44\n",
      "0.62 0.38\n",
      "0.73 0.27\n",
      "0.7 0.3\n",
      "0.59 0.41\n",
      "0.65 0.35\n",
      "0.63 0.37\n",
      "0.53 0.47\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.57 0.43\n",
      "0.54 0.46\n",
      "0.69 0.31\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.65 0.35\n",
      "0.6 0.4\n",
      "0.61 0.39\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.7 0.3\n",
      "0.56 0.44\n",
      "0.5 0.5\n",
      "0.64 0.36\n",
      "0.57 0.43\n",
      "0.56 0.44\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.7 0.3\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.63 0.37\n",
      "0.51 0.49\n",
      "0.69 0.31\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.53 0.47\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.66 0.34\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.66 0.34\n",
      "0.62 0.38\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.55 0.45\n",
      "0.61 0.39\n",
      "0.52 0.48\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.52 0.48\n",
      "0.6 0.4\n",
      "0.66 0.34\n",
      "0.67 0.33\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.56 0.44\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.66 0.34\n",
      "0.66 0.34\n",
      "0.49 0.51\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.49 0.51\n",
      "0.51 0.49\n",
      "0.62 0.38\n",
      "0.54 0.46\n",
      "0.6 0.4\n",
      "0.56 0.44\n",
      "0.49 0.51\n",
      "0.57 0.43\n",
      "0.49 0.51\n",
      "0.67 0.33\n",
      "0.64 0.36\n",
      "0.55 0.45\n",
      "0.49 0.51\n",
      "0.5 0.5\n",
      "0.55 0.45\n",
      "0.65 0.35\n",
      "0.66 0.34\n",
      "0.67 0.33\n",
      "0.56 0.44\n",
      "0.61 0.39\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.52 0.48\n",
      "0.63 0.37\n",
      "0.63 0.37\n",
      "0.61 0.39\n",
      "0.56 0.44\n",
      "0.64 0.36\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.68 0.32\n",
      "0.65 0.35\n",
      "0.53 0.47\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.67 0.33\n",
      "0.58 0.42\n",
      "0.62 0.38\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.58 0.42\n",
      "0.5 0.5\n",
      "0.54 0.46\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.65 0.35\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.65 0.35\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.58 0.42\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.59 0.41\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.51 0.49\n",
      "0.69 0.31\n",
      "0.53 0.47\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.69 0.31\n",
      "0.55 0.45\n",
      "0.66 0.34\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.49 0.51\n",
      "0.61 0.39\n",
      "0.53 0.47\n",
      "0.66 0.34\n",
      "0.49 0.51\n",
      "0.53 0.47\n",
      "0.57 0.43\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.54 0.46\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.53 0.47\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.64 0.36\n",
      "0.68 0.32\n",
      "0.49 0.51\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.56 0.44\n",
      "0.52 0.48\n",
      "0.69 0.31\n",
      "0.61 0.39\n",
      "0.57 0.43\n",
      "0.49 0.51\n",
      "0.66 0.34\n",
      "0.54 0.46\n",
      "0.53 0.47\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.61 0.39\n",
      "0.55 0.45\n",
      "0.61 0.39\n",
      "0.71 0.29\n",
      "0.62 0.38\n",
      "0.64 0.36\n",
      "0.62 0.38\n",
      "0.58 0.42\n",
      "0.62 0.38\n",
      "0.61 0.39\n",
      "0.62 0.38\n",
      "0.61 0.39\n",
      "0.68 0.32\n",
      "0.66 0.34\n",
      "0.49 0.51\n",
      "0.68 0.32\n",
      "0.52 0.48\n",
      "0.59 0.41\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.62 0.38\n",
      "0.54 0.46\n",
      "0.5 0.5\n",
      "0.62 0.38\n",
      "0.62 0.38\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.55 0.45\n",
      "0.63 0.37\n",
      "0.64 0.36\n",
      "0.59 0.41\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.7 0.3\n",
      "0.5 0.5\n",
      "0.62 0.38\n",
      "0.56 0.44\n",
      "0.57 0.43\n",
      "0.62 0.38\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.53 0.47\n",
      "0.49 0.51\n",
      "0.51 0.49\n",
      "0.5 0.5\n",
      "0.64 0.36\n",
      "0.53 0.47\n",
      "0.65 0.35\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.61 0.39\n",
      "0.62 0.38\n",
      "0.53 0.47\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.68 0.32\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.5 0.5\n",
      "0.69 0.31\n",
      "0.56 0.44\n",
      "0.54 0.46\n",
      "0.57 0.43\n",
      "0.61 0.39\n",
      "0.63 0.37\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.65 0.35\n",
      "0.65 0.35\n",
      "0.53 0.47\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.55 0.45\n",
      "0.49 0.51\n",
      "0.63 0.37\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.66 0.34\n",
      "0.55 0.45\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.57 0.43\n",
      "0.58 0.42\n",
      "0.63 0.37\n",
      "0.58 0.42\n",
      "0.6 0.4\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.55 0.45\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.51 0.49\n",
      "0.56 0.44\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.55 0.45\n",
      "0.58 0.42\n",
      "0.66 0.34\n",
      "0.53 0.47\n",
      "0.6 0.4\n",
      "0.6 0.4\n",
      "0.63 0.37\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.59 0.41\n",
      "0.61 0.39\n",
      "0.63 0.37\n",
      "0.7 0.3\n",
      "0.54 0.46\n",
      "0.53 0.47\n",
      "0.57 0.43\n",
      "0.65 0.35\n",
      "0.51 0.49\n",
      "0.61 0.39\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.53 0.47\n",
      "0.6 0.4\n",
      "0.54 0.46\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.71 0.29\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.56 0.44\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.57 0.43\n",
      "0.57 0.43\n",
      "0.67 0.33\n",
      "0.55 0.45\n",
      "0.51 0.49\n",
      "0.58 0.42\n",
      "0.57 0.43\n",
      "0.7 0.3\n",
      "0.54 0.46\n",
      "0.54 0.46\n",
      "0.49 0.51\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.65 0.35\n",
      "0.63 0.37\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.68 0.32\n",
      "0.55 0.45\n",
      "0.54 0.46\n",
      "0.51 0.49\n",
      "0.68 0.32\n",
      "0.65 0.35\n",
      "0.57 0.43\n",
      "0.66 0.34\n",
      "0.63 0.37\n",
      "0.64 0.36\n",
      "0.57 0.43\n",
      "0.49 0.51\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.56 0.44\n",
      "0.62 0.38\n",
      "0.58 0.42\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.6 0.4\n",
      "0.61 0.39\n",
      "0.64 0.36\n",
      "0.51 0.49\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.51 0.49\n",
      "0.57 0.43\n",
      "0.69 0.31\n",
      "0.6 0.4\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.59 0.41\n",
      "0.69 0.31\n",
      "0.53 0.47\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.59 0.41\n",
      "0.51 0.49\n",
      "0.62 0.38\n",
      "0.49 0.51\n",
      "0.65 0.35\n",
      "0.69 0.31\n",
      "0.55 0.45\n",
      "0.6 0.4\n",
      "0.63 0.37\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.53 0.47\n",
      "0.7 0.3\n",
      "0.56 0.44\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.62 0.38\n",
      "0.64 0.36\n",
      "0.68 0.32\n",
      "0.64 0.36\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.53 0.47\n",
      "0.51 0.49\n",
      "0.69 0.31\n",
      "0.49 0.51\n",
      "0.62 0.38\n",
      "0.55 0.45\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.64 0.36\n",
      "0.53 0.47\n",
      "0.66 0.34\n",
      "0.71 0.29\n",
      "0.53 0.47\n",
      "0.64 0.36\n",
      "0.55 0.45\n",
      "0.51 0.49\n",
      "0.55 0.45\n",
      "0.55 0.45\n",
      "0.5 0.5\n",
      "0.56 0.44\n",
      "0.49 0.51\n",
      "0.56 0.44\n",
      "0.7 0.3\n",
      "0.68 0.32\n",
      "0.52 0.48\n",
      "0.56 0.44\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.69 0.31\n",
      "0.66 0.34\n",
      "0.69 0.31\n",
      "0.65 0.35\n",
      "0.51 0.49\n",
      "0.64 0.36\n",
      "0.51 0.49\n",
      "0.68 0.32\n",
      "0.52 0.48\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.55 0.45\n",
      "0.62 0.38\n",
      "0.54 0.46\n",
      "0.58 0.42\n",
      "0.64 0.36\n",
      "0.62 0.38\n",
      "0.56 0.44\n",
      "0.65 0.35\n",
      "0.59 0.41\n",
      "0.64 0.36\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.49 0.51\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.62 0.38\n",
      "0.57 0.43\n",
      "0.7 0.3\n",
      "0.68 0.32\n",
      "0.54 0.46\n",
      "0.51 0.49\n",
      "0.67 0.33\n",
      "0.51 0.49\n",
      "0.49 0.51\n",
      "0.65 0.35\n",
      "0.51 0.49\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.49 0.51\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.64 0.36\n",
      "0.69 0.31\n",
      "0.66 0.34\n",
      "0.68 0.32\n",
      "0.55 0.45\n",
      "0.53 0.47\n",
      "0.68 0.32\n",
      "0.61 0.39\n",
      "0.65 0.35\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.62 0.38\n",
      "0.63 0.37\n",
      "0.55 0.45\n",
      "0.5 0.5\n",
      "0.53 0.47\n",
      "0.61 0.39\n",
      "0.64 0.36\n",
      "0.64 0.36\n",
      "0.55 0.45\n",
      "0.52 0.48\n",
      "0.66 0.34\n",
      "0.56 0.44\n",
      "0.58 0.42\n",
      "0.53 0.47\n",
      "0.59 0.41\n",
      "0.65 0.35\n",
      "0.7 0.3\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.62 0.38\n",
      "0.53 0.47\n",
      "0.65 0.35\n",
      "0.57 0.43\n",
      "0.52 0.48\n",
      "0.63 0.37\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.68 0.32\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.66 0.34\n",
      "0.65 0.35\n",
      "0.49 0.51\n",
      "0.59 0.41\n",
      "0.63 0.37\n",
      "0.64 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67 0.33\n",
      "0.52 0.48\n",
      "0.67 0.33\n",
      "0.6 0.4\n",
      "0.69 0.31\n",
      "0.54 0.46\n",
      "0.54 0.46\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.66 0.34\n",
      "0.58 0.42\n",
      "0.49 0.51\n",
      "0.62 0.38\n",
      "0.54 0.46\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.62 0.38\n",
      "0.54 0.46\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.67 0.33\n",
      "0.52 0.48\n",
      "0.52 0.48\n",
      "0.71 0.29\n",
      "0.6 0.4\n",
      "0.56 0.44\n",
      "0.56 0.44\n",
      "0.51 0.49\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.58 0.42\n",
      "0.66 0.34\n",
      "0.53 0.47\n",
      "0.65 0.35\n",
      "0.57 0.43\n",
      "0.63 0.37\n",
      "0.69 0.31\n",
      "0.7 0.3\n",
      "0.52 0.48\n",
      "0.53 0.47\n",
      "0.49 0.51\n",
      "0.59 0.41\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.59 0.41\n",
      "0.6 0.4\n",
      "0.57 0.43\n",
      "0.56 0.44\n",
      "0.61 0.39\n",
      "0.56 0.44\n",
      "0.65 0.35\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.58 0.42\n",
      "0.61 0.39\n",
      "0.56 0.44\n",
      "0.58 0.42\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.63 0.37\n",
      "0.55 0.45\n",
      "0.53 0.47\n",
      "0.53 0.47\n",
      "0.55 0.45\n",
      "0.64 0.36\n",
      "0.68 0.32\n",
      "0.56 0.44\n",
      "0.57 0.43\n",
      "0.67 0.33\n",
      "0.58 0.42\n",
      "0.59 0.41\n",
      "0.52 0.48\n",
      "0.55 0.45\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.5 0.5\n",
      "0.54 0.46\n",
      "0.54 0.46\n",
      "0.59 0.41\n",
      "0.67 0.33\n",
      "0.59 0.41\n",
      "0.57 0.43\n",
      "0.55 0.45\n",
      "0.55 0.45\n",
      "0.65 0.35\n",
      "0.53 0.47\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.49 0.51\n",
      "0.52 0.48\n",
      "0.62 0.38\n",
      "0.66 0.34\n",
      "0.55 0.45\n",
      "0.67 0.33\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.64 0.36\n",
      "0.49 0.51\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.56 0.44\n",
      "0.54 0.46\n",
      "0.65 0.35\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.63 0.37\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.52 0.48\n",
      "0.51 0.49\n",
      "0.54 0.46\n",
      "0.6 0.4\n",
      "0.64 0.36\n",
      "0.64 0.36\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.65 0.35\n",
      "0.57 0.43\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.52 0.48\n",
      "0.5 0.5\n",
      "0.55 0.45\n",
      "0.61 0.39\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.67 0.33\n",
      "0.5 0.5\n",
      "0.67 0.33\n",
      "0.6 0.4\n",
      "0.64 0.36\n",
      "0.56 0.44\n",
      "0.63 0.37\n",
      "0.53 0.47\n",
      "0.55 0.45\n",
      "0.52 0.48\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.62 0.38\n",
      "0.61 0.39\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.62 0.38\n",
      "0.68 0.32\n",
      "0.51 0.49\n",
      "0.69 0.31\n",
      "0.62 0.38\n",
      "0.55 0.45\n",
      "0.62 0.38\n",
      "0.58 0.42\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.67 0.33\n",
      "0.58 0.42\n",
      "0.5 0.5\n",
      "0.55 0.45\n",
      "0.67 0.33\n",
      "0.65 0.35\n",
      "0.52 0.48\n",
      "0.57 0.43\n",
      "0.63 0.37\n",
      "0.6 0.4\n",
      "0.69 0.31\n",
      "0.64 0.36\n",
      "0.65 0.35\n",
      "0.64 0.36\n",
      "0.63 0.37\n",
      "0.56 0.44\n",
      "0.49 0.51\n",
      "0.53 0.47\n",
      "0.54 0.46\n",
      "0.49 0.51\n",
      "0.6 0.4\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.49 0.51\n",
      "0.56 0.44\n",
      "0.56 0.44\n",
      "0.5 0.5\n",
      "0.49 0.51\n",
      "0.62 0.38\n",
      "0.51 0.49\n",
      "0.68 0.32\n",
      "0.6 0.4\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.64 0.36\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.57 0.43\n",
      "0.68 0.32\n",
      "0.52 0.48\n",
      "0.54 0.46\n",
      "0.53 0.47\n",
      "0.66 0.34\n",
      "0.52 0.48\n",
      "0.57 0.43\n",
      "0.59 0.41\n",
      "0.62 0.38\n",
      "0.5 0.5\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.49 0.51\n",
      "0.52 0.48\n",
      "0.68 0.32\n",
      "0.56 0.44\n",
      "0.59 0.41\n",
      "0.5 0.5\n",
      "0.69 0.31\n",
      "0.65 0.35\n",
      "0.65 0.35\n",
      "0.68 0.32\n",
      "0.6 0.4\n",
      "0.51 0.49\n",
      "0.6 0.4\n",
      "0.55 0.45\n",
      "0.55 0.45\n",
      "0.49 0.51\n",
      "0.62 0.38\n",
      "0.65 0.35\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.55 0.45\n",
      "0.51 0.49\n",
      "0.5 0.5\n",
      "0.57 0.43\n",
      "0.53 0.47\n",
      "0.72 0.28\n",
      "0.5 0.5\n",
      "0.5 0.5\n",
      "0.59 0.41\n",
      "0.53 0.47\n",
      "0.56 0.44\n",
      "0.62 0.38\n",
      "0.64 0.36\n",
      "0.65 0.35\n",
      "0.55 0.45\n",
      "0.59 0.41\n",
      "0.65 0.35\n",
      "0.5 0.5\n",
      "0.67 0.33\n",
      "0.6 0.4\n",
      "0.69 0.31\n",
      "0.54 0.46\n",
      "0.52 0.48\n",
      "0.67 0.33\n",
      "0.62 0.38\n",
      "0.61 0.39\n",
      "0.64 0.36\n",
      "0.52 0.48\n",
      "0.64 0.36\n",
      "0.55 0.45\n",
      "0.65 0.35\n",
      "0.66 0.34\n",
      "0.62 0.38\n",
      "0.63 0.37\n",
      "0.68 0.32\n",
      "0.56 0.44\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.57 0.43\n",
      "0.6 0.4\n",
      "0.57 0.43\n",
      "0.56 0.44\n",
      "0.68 0.32\n",
      "0.53 0.47\n",
      "0.52 0.48\n",
      "0.61 0.39\n",
      "0.6 0.4\n",
      "0.57 0.43\n",
      "0.54 0.46\n",
      "0.65 0.35\n",
      "0.61 0.39\n",
      "0.55 0.45\n",
      "0.61 0.39\n",
      "0.54 0.46\n",
      "0.5 0.5\n",
      "0.66 0.34\n",
      "0.66 0.34\n",
      "0.6 0.4\n",
      "0.56 0.44\n",
      "0.53 0.47\n",
      "0.55 0.45\n",
      "0.59 0.41\n",
      "0.59 0.41\n",
      "0.64 0.36\n",
      "0.58 0.42\n",
      "0.53 0.47\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "for prediction in predictions:\n",
    "    print(round(prediction[0], 2), round(prediction[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04  0.04]\n",
      "0.040000000000000036\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction = np.array([0.48, 0.52])\n",
    "old_prediction = np.array([0.52, 0.48])\n",
    "\n",
    "prediction_difference = new_prediction - old_prediction\n",
    "print(prediction_difference)\n",
    "\n",
    "std = np.std(prediction_difference)\n",
    "mean = np.mean(prediction_difference)\n",
    "\n",
    "print(std)\n",
    "print(mean)\n",
    "\n",
    "# std/mean\n",
    "\n",
    "sum((prediction_difference - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5285661 , 0.4714338 ],\n",
       "       [0.6277289 , 0.3722711 ],\n",
       "       [0.62586445, 0.37413546],\n",
       "       ...,\n",
       "       [0.64191854, 0.3580815 ],\n",
       "       [0.5827341 , 0.41726595],\n",
       "       [0.53372526, 0.4662747 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_prediction = model.predict(x_test)\n",
    "old_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49637026, 0.50362974],\n",
       "       [0.63519156, 0.36480844],\n",
       "       [0.6940631 , 0.3059369 ],\n",
       "       ...,\n",
       "       [0.6900633 , 0.3099367 ],\n",
       "       [0.6020439 , 0.3979561 ],\n",
       "       [0.49627852, 0.5037215 ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction = model.predict(x_test)\n",
    "new_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03219587  0.03219596]\n",
      " [ 0.00746268 -0.00746265]\n",
      " [ 0.06819868 -0.06819856]\n",
      " ...\n",
      " [ 0.04814476 -0.04814479]\n",
      " [ 0.01930982 -0.01930985]\n",
      " [-0.03744674  0.03744677]]\n",
      "0.04246777\n"
     ]
    }
   ],
   "source": [
    "prediction_difference = new_prediction - old_prediction\n",
    "print(prediction_difference)\n",
    "\n",
    "std = np.std(prediction_difference)\n",
    "mean = np.mean(prediction_difference)\n",
    "\n",
    "print(std)\n",
    "# print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036413886"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.std(prediction_difference, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3841858e-07"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.mean(prediction_difference, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03219591, 0.00746267, 0.06819862, ..., 0.04814477, 0.01930983,\n",
       "       0.03744675], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(prediction_difference, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03219587,  0.03219596],\n",
       "       [ 0.00746268, -0.00746265]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(prediction_difference[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03219591, 0.00746267], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(prediction_difference[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019829288"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.std(prediction_difference[:2], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(new_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49637026, 0.50362974], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714338"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_prediction[0][np.argmax(new_prediction[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.434959</td>\n",
       "      <td>0.827087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.225807</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37.490726</td>\n",
       "      <td>0.770804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35.019646</td>\n",
       "      <td>0.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.181820</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>34.833820</td>\n",
       "      <td>0.830904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race        age       sex\n",
       "0     0  32.434959  0.827087\n",
       "1     1  38.225807  0.935484\n",
       "2     2  37.490726  0.770804\n",
       "3     3  35.019646  0.838900\n",
       "4     4  33.181820  0.818182\n",
       "5     5  34.833820  0.830904"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "            df.groupby([\"race\"])[[\n",
    "                \"age\", \"sex\"\n",
    "            ]]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,2,\"yes\"],[1,5,\"k\"],[1,8,\"no\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1    2\n",
       "0  1  2  yes\n",
       "1  1  5    k\n",
       "2  1  8   no"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>mod</th>\n",
       "      <th>hello</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>k</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0 mod  hello\n",
       "0  1   k      5"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "            df.groupby([0])\n",
    "#             .mean()\n",
    "#             .agg({1:'sum', 2:'max'})\n",
    "    .agg(mod = (2, lambda x: x.value_counts().index[0]),\n",
    "         hello = (1, 'mean')\n",
    "        )\n",
    "            .reset_index()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert yesnoyes to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'yesnoyes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1540\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: complex() arg is a malformed string",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8q/rrpjvjbd3fq288lrgfdtz9f00000gn/T/ipykernel_35659/1345048182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3368\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11116\u001b[0m         )\n\u001b[1;32m  11117\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11120\u001b[0m         \u001b[0;31m# pandas\\core\\generic.py:10924: error: Cannot assign to a method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10726\u001b[0m         return self._stat_function(\n\u001b[0;32m> 10727\u001b[0;31m             \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10728\u001b[0m         )\n\u001b[1;32m  10729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_by_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10711\u001b[0m         return self._reduce(\n\u001b[0;32m> 10712\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10713\u001b[0m         )\n\u001b[1;32m  10714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4180\u001b[0m                 )\n\u001b[1;32m   4181\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdatetimelike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ml_bias_explainability-a4D5zUqS/lib/python3.7/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m                 \u001b[0;31m# e.g. \"foo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not convert {x} to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert yesnoyes to numeric"
     ]
    }
   ],
   "source": [
    "np.mean(df[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExplainBias().check_probability_of_outcomes(model, x_test, y_test, 0.1)\n",
    "# tensorboard, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_threshold = 10\n",
    "verbose = False\n",
    "predictions_df, aggregate_predictions_df = ExplainBias().modify_input_and_observe_effect_on_output(model, x_test, df, columns_to_remove, unique_values_threshold, verbose)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_explainability_bias",
   "language": "python",
   "name": "ml_explainability_bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
